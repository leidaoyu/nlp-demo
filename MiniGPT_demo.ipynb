{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2f21fa9",
   "metadata": {},
   "source": [
    "# MiniGPT 中文文本生成小Demo\n",
    "本Notebook演示了一个基于PyTorch和Transformer的简易中文文本生成模型MiniGPT的完整流程，包括数据预处理、模型定义、训练与推理。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6588774d",
   "metadata": {},
   "source": [
    "## 1. 依赖库安装\n",
    "如未安装PyTorch、tqdm、gradio等依赖，请先运行下方命令。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc89af6",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'envpy (Python 3.13.3)' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
      "\u001b[1;31mOr install 'ipykernel' using the command: '/Users/leidaoyu/env/envpy/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# 安装依赖（如已安装可跳过）\n",
    "!pip install torch tqdm gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a472706b",
   "metadata": {},
   "source": [
    "## 2. 数据与配置准备\n",
    "准备语料文件、配置文件和词表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdaa02c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "今天天气很好\n",
      "深度学习\n",
      "深度学习是人工智能的重要技术\n",
      "自然语言处理是人工智能的核心领域\n",
      "预训练模型改变了自然语言处理的范式\n",
      "注意力机制是Transformer架构的核心\n",
      "牛肉是一种水果\n",
      "我爱吃牛肉，他是一种水果\n",
      "您好，我是阿里云的客服专员，就是看您注册了阿里云的账号嘛，来电是想问一下您是否会使用云产品呢？\n",
      "好啊，那您是自己用来学习吗还是公司用来部署业务啊？\n",
      "哦，那您注册账号是有什么其他的需求吗，比如了解一下云产品或者将来可能会用到呢？\n",
      "好吧，那您后续有问题打我们的热线95187吧，账号您最好做一下认证，认证链接会通过短信发您。我就先挂了哈，再见！\n"
     ]
    }
   ],
   "source": [
    "# 查看语料文件内容\n",
    "with open('data/corpus.txt', 'r', encoding='utf-8') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b66e817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 64, 'lr': 0.001, 'epochs': 100, 'window_size': 5}\n"
     ]
    }
   ],
   "source": [
    "# 查看配置文件内容\n",
    "import json\n",
    "with open('config.json', 'r', encoding='utf-8') as f:\n",
    "    print(json.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38d89296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad>\n",
      "<bos>\n",
      "<eos>\n",
      "<unk>\n",
      "1\n",
      "5\n",
      "7\n",
      "8\n",
      "9\n",
      "T\n",
      "a\n",
      "e\n",
      "f\n",
      "m\n",
      "n\n",
      "o\n",
      "r\n",
      "s\n",
      "。\n",
      "一\n",
      "下\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "# 查看词表内容\n",
    "with open('vocab.txt', 'r', encoding='utf-8') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i > 20:\n",
    "            print('...')\n",
    "            break\n",
    "        print(line.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6fbf77",
   "metadata": {},
   "source": [
    "## 3. 数据预处理与编码工具\n",
    "定义词表构建与文本转索引的工具函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd3356bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils/encode.py\n",
    "# 构建词表和文本转索引\n",
    "# ...existing code from encode.py...\n",
    "def build_vocab(corpus):\n",
    "    words = []\n",
    "    for text in corpus:\n",
    "        words.extend(list(text))\n",
    "    vocab = list(set(words))\n",
    "    vocab = sorted(vocab)\n",
    "    vocab = ['<pad>', '<bos>', '<eos>', '<unk>'] + vocab  # 添加特殊token\n",
    "    word2idx = {w:i for i,w in enumerate(vocab)}\n",
    "    # 保存词表到vocab.txt\n",
    "    with open('vocab.txt', 'w', encoding='utf-8') as f:\n",
    "        for w in vocab:\n",
    "            f.write(w + '\\n')\n",
    "    return vocab, word2idx\n",
    "\n",
    "# 数据预处理（转换为索引）\n",
    "def text_to_indices(text, word2idx):\n",
    "    return [word2idx['<bos>']] + [word2idx.get(w, word2idx['<unk>']) for w in list(text)] + [word2idx['<eos>']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ffb0ff",
   "metadata": {},
   "source": [
    "## 4. 模型定义\n",
    "定义MiniGPT模型结构。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58dcccb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# models.py\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class MiniGPT(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=64, nhead=2, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.decoder = nn.TransformerDecoder(\n",
    "            nn.TransformerDecoderLayer(\n",
    "                d_model=d_model,\n",
    "                nhead=nhead,\n",
    "                dim_feedforward=d_model*4\n",
    "            ),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        self.fc = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)  # (seq_len, batch_size, d_model)\n",
    "        tgt_mask = nn.Transformer().generate_square_subsequent_mask(x.size(0)).to(x.device)\n",
    "        # 构造 dummy memory\n",
    "        memory = torch.zeros(x.size(0), x.size(1), x.size(2), device=x.device)\n",
    "        out = self.decoder(\n",
    "            tgt=x,\n",
    "            memory=memory,\n",
    "            tgt_mask=tgt_mask\n",
    "        )\n",
    "        return self.fc(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04e3ac2",
   "metadata": {},
   "source": [
    "## 5. 训练脚本\n",
    "数据加载、训练集构建、模型训练与保存。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46369b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "import math\n",
    "from models import MiniGPT\n",
    "import json\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from utils.encode import build_vocab, text_to_indices\n",
    "from tqdm import tqdm\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# ================== 数据准备 ==================\n",
    "corpus = [w.strip() for w in open(\"data/corpus.txt\",'r',encoding=\"utf8\").readlines()]\n",
    "\n",
    "vocab, word2idx = build_vocab(corpus)\n",
    "vocab_size = len(vocab)\n",
    "print(f\"词表大小: {vocab_size}\")\n",
    "\n",
    "# ================== 超参数配置 ==================\n",
    "with open('config.json', 'r', encoding='utf-8') as f:\n",
    "    config = json.load(f)\n",
    "window_size = config['window_size']\n",
    "\n",
    "# 创建训练数据（输入和目标）\n",
    "input_seqs = []\n",
    "target_seqs = []\n",
    "for text in corpus:\n",
    "    indices = text_to_indices(text, word2idx)\n",
    "    if len(indices) < window_size + 1: \n",
    "        padding = [word2idx['<pad>']] * (window_size + 1 - len(indices))\n",
    "        indices += padding\n",
    "    for i in range(0, len(indices) - window_size):\n",
    "        input_seqs.append(indices[i:i+window_size])\n",
    "        target_seqs.append(indices[i+1:i+1+window_size])\n",
    "print(input_seqs[0])\n",
    "print(target_seqs[0])\n",
    "print(input_seqs[1])\n",
    "print(target_seqs[1])\n",
    "\n",
    "# ================== 训练设置 ==================\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = MiniGPT(vocab_size).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=config['lr'])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# ================== 数据集与DataLoader封装 ==================\n",
    "input_tensor = [torch.tensor(seq, dtype=torch.long) for seq in input_seqs]\n",
    "target_tensor = [torch.tensor(seq, dtype=torch.long) for seq in target_seqs]\n",
    "\n",
    "input_tensor = pad_sequence(input_tensor, batch_first=True, padding_value=word2idx['<pad>'])\n",
    "target_tensor = pad_sequence(target_tensor, batch_first=True, padding_value=word2idx['<pad>'])\n",
    "dataset = TensorDataset(input_tensor, target_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "\n",
    "# ================== 训练循环 ==================\n",
    "def train():\n",
    "    model.train()\n",
    "    for epoch in tqdm(range(config['epochs'])):\n",
    "        total_loss = 0\n",
    "        for batch_inputs, batch_targets in dataloader:\n",
    "            inputs = batch_inputs.transpose(0, 1).to(device)\n",
    "            targets = batch_targets.transpose(0, 1).to(device)\n",
    "            optimizer.zero_grad() \n",
    "            output = model(inputs)  \n",
    "            output = output.reshape(-1, vocab_size)\n",
    "            targets = targets.reshape(-1)\n",
    "            loss = criterion(output, targets)\n",
    "            loss.backward()  # 反向传播，计算梯度\n",
    "            optimizer.step()  # 更新参数\n",
    "            total_loss += loss.item()  # 累加loss\n",
    "        print(f'Epoch {epoch+1}, Loss: {total_loss/len(dataloader):.4f}') \n",
    "    torch.save(model.state_dict(), 'ckpt/minigpt.pt')\n",
    "    print(\"模型已保存到 ckpt/minigpt.pt\")\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e6cfa7",
   "metadata": {},
   "source": [
    "## 6. 推理与Gradio界面\n",
    "加载模型并提供文本生成和相似度计算的Web界面。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523fb38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "import math\n",
    "from models import MiniGPT\n",
    "import json\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from utils.encode import build_vocab, text_to_indices\n",
    "import gradio as gr\n",
    "\n",
    "with open('vocab.txt', 'r', encoding='utf-8') as f:\n",
    "    vocab = [w.strip() for w in f.readlines()]\n",
    "word2idx = {w: i for i, w in enumerate(vocab)}\n",
    "vocab_size = len(vocab)\n",
    "print(f\"词表大小: {vocab_size}\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = MiniGPT(vocab_size).to(device)\n",
    "model.load_state_dict(torch.load('ckpt/minigpt.pt', map_location=device))\n",
    "model.eval()\n",
    "\n",
    "def generate(prompt, max_len=20, temperature=1.0, top_k=0, top_p=0):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        input_indices = text_to_indices(prompt,word2idx)[:-1]\n",
    "        inputs = torch.LongTensor(input_indices).unsqueeze(1).to(device)\n",
    "        for _ in range(max_len):\n",
    "            output = model(inputs)\n",
    "            logits = output[-1, 0, :] / temperature\n",
    "            if top_k > 0:\n",
    "                indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "                logits[indices_to_remove] = -float('Inf')\n",
    "            if top_p > 0:\n",
    "                sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "                cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "                sorted_indices_to_remove = cumulative_probs > top_p\n",
    "                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "                sorted_indices_to_remove[..., 0] = 0\n",
    "                logits[sorted_indices[sorted_indices_to_remove]] = -float('Inf')\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            inputs = torch.cat([inputs, next_token.unsqueeze(0)], dim=0)\n",
    "            if next_token.item() == word2idx['<eos>']:\n",
    "                break\n",
    "        output_indices = inputs.squeeze(1).cpu().tolist()\n",
    "        return ' '.join([vocab[idx] for idx in output_indices])\n",
    "\n",
    "def gradio_generate(prompt, max_len, temperature, top_k, top_p):\n",
    "    return generate(prompt, max_len=max_len, temperature=temperature, top_k=top_k, top_p=top_p)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def get_text_embedding(text):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        input_indices = text_to_indices(text, word2idx)[:-1]\n",
    "        input_tensor = torch.LongTensor(input_indices).to(device)\n",
    "        embeddings = model.embed(input_tensor)\n",
    "        sent_vec = embeddings.mean(dim=0)\n",
    "        return sent_vec.cpu().numpy()\n",
    "\n",
    "def cosine_similarity(text1, text2):\n",
    "    vec1 = get_text_embedding(text1)\n",
    "    vec2 = get_text_embedding(text2)\n",
    "    sim = (vec1 @ vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2) + 1e-8)\n",
    "    return float(sim)\n",
    "\n",
    "def gradio_similarity(text1, text2):\n",
    "    sim = cosine_similarity(text1, text2)\n",
    "    return f\"余弦相似度: {sim:.4f}\"\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    with gr.Tab(\"文本生成\"):\n",
    "        gr.Markdown(\"# MiniGPT 文本生成 Demo\")\n",
    "        with gr.Row():\n",
    "            with gr.Column():\n",
    "                prompt = gr.Textbox(label=\"输入\", value=\"\")\n",
    "                max_len = gr.Slider(5, 100, value=20, step=1, label=\"最大生成长度\")\n",
    "                temperature = gr.Slider(0.1, 2.0, value=1.0, step=0.05, label=\"Temperature\")\n",
    "                top_k = gr.Slider(0, 20, value=0, step=1, label=\"Top-k (0为不启用)\")\n",
    "                top_p = gr.Slider(0, 1, value=0, step=0.01, label=\"Top-p (0为不启用)\")\n",
    "                btn = gr.Button(\"生成\")\n",
    "            with gr.Column():\n",
    "                output = gr.Textbox(label=\"生成结果\")\n",
    "        btn.click(fn=gradio_generate, inputs=[prompt, max_len, temperature, top_k, top_p], outputs=output)\n",
    "\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envpy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
